{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from linearmodels import PanelOLS\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 145063 entries, 0 to 145062\n",
      "Columns: 551 entries, Page to 2016-12-31\n",
      "dtypes: float64(550), object(1)\n",
      "memory usage: 609.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page</th>\n",
       "      <th>2015-07-01</th>\n",
       "      <th>2015-07-02</th>\n",
       "      <th>2015-07-03</th>\n",
       "      <th>2015-07-04</th>\n",
       "      <th>2015-07-05</th>\n",
       "      <th>2015-07-06</th>\n",
       "      <th>2015-07-07</th>\n",
       "      <th>2015-07-08</th>\n",
       "      <th>2015-07-09</th>\n",
       "      <th>...</th>\n",
       "      <th>2016-12-22</th>\n",
       "      <th>2016-12-23</th>\n",
       "      <th>2016-12-24</th>\n",
       "      <th>2016-12-25</th>\n",
       "      <th>2016-12-26</th>\n",
       "      <th>2016-12-27</th>\n",
       "      <th>2016-12-28</th>\n",
       "      <th>2016-12-29</th>\n",
       "      <th>2016-12-30</th>\n",
       "      <th>2016-12-31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2NE1_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2PM_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3C_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4minute_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>35.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>48.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 551 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Page  2015-07-01  2015-07-02  \\\n",
       "0            2NE1_zh.wikipedia.org_all-access_spider        18.0        11.0   \n",
       "1             2PM_zh.wikipedia.org_all-access_spider        11.0        14.0   \n",
       "2              3C_zh.wikipedia.org_all-access_spider         1.0         0.0   \n",
       "3         4minute_zh.wikipedia.org_all-access_spider        35.0        13.0   \n",
       "4  52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...         NaN         NaN   \n",
       "\n",
       "   2015-07-03  2015-07-04  2015-07-05  2015-07-06  2015-07-07  2015-07-08  \\\n",
       "0         5.0        13.0        14.0         9.0         9.0        22.0   \n",
       "1        15.0        18.0        11.0        13.0        22.0        11.0   \n",
       "2         1.0         1.0         0.0         4.0         0.0         3.0   \n",
       "3        10.0        94.0         4.0        26.0        14.0         9.0   \n",
       "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "   2015-07-09  ...  2016-12-22  2016-12-23  2016-12-24  2016-12-25  \\\n",
       "0        26.0  ...        32.0        63.0        15.0        26.0   \n",
       "1        10.0  ...        17.0        42.0        28.0        15.0   \n",
       "2         4.0  ...         3.0         1.0         1.0         7.0   \n",
       "3        11.0  ...        32.0        10.0        26.0        27.0   \n",
       "4         NaN  ...        48.0         9.0        25.0        13.0   \n",
       "\n",
       "   2016-12-26  2016-12-27  2016-12-28  2016-12-29  2016-12-30  2016-12-31  \n",
       "0        14.0        20.0        22.0        19.0        18.0        20.0  \n",
       "1         9.0        30.0        52.0        45.0        26.0        20.0  \n",
       "2         4.0         4.0         6.0         3.0         4.0        17.0  \n",
       "3        16.0        11.0        17.0        19.0        10.0        11.0  \n",
       "4         3.0        11.0        27.0        13.0        36.0        10.0  \n",
       "\n",
       "[5 rows x 551 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('C:\\\\Users\\\\Shadeh\\\\Pictures\\\\pyfolder\\\\train_1.csv')\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page</th>\n",
       "      <th>Id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!vote_en.wikipedia.org_all-access_all-agents</td>\n",
       "      <td>bf4edcf969af</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!vote_en.wikipedia.org_all-access_all-agents</td>\n",
       "      <td>929ed2bf52b9</td>\n",
       "      <td>2017-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>!vote_en.wikipedia.org_all-access_all-agents</td>\n",
       "      <td>ff29d0f51d5c</td>\n",
       "      <td>2017-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>!vote_en.wikipedia.org_all-access_all-agents</td>\n",
       "      <td>e98873359be6</td>\n",
       "      <td>2017-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>!vote_en.wikipedia.org_all-access_all-agents</td>\n",
       "      <td>fa012434263a</td>\n",
       "      <td>2017-01-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Page            Id        date\n",
       "0  !vote_en.wikipedia.org_all-access_all-agents  bf4edcf969af  2017-01-01\n",
       "1  !vote_en.wikipedia.org_all-access_all-agents  929ed2bf52b9  2017-01-02\n",
       "2  !vote_en.wikipedia.org_all-access_all-agents  ff29d0f51d5c  2017-01-03\n",
       "3  !vote_en.wikipedia.org_all-access_all-agents  e98873359be6  2017-01-04\n",
       "4  !vote_en.wikipedia.org_all-access_all-agents  fa012434263a  2017-01-05"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = pd.read_csv('C:\\\\Users\\\\Shadeh\\\\Pictures\\\\pyfolder\\\\key_1.csv')\n",
    "key_split = key.copy()\n",
    "key_split['date'] = key_split.Page.apply(lambda a: a[-10:])\n",
    "key_split['Page'] = key_split.Page.apply(lambda a: a[:-11])\n",
    "key_split.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_test(y_true, y_pred):\n",
    "    denominator = (y_true+y_pred)/200.0\n",
    "    diff = np.abs(y_true-y_pred)/denominator\n",
    "    diff[denominator==0] = 0.0\n",
    "    return np.mean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2NE1_zh.wikipedia.org_all-access_spider\n",
      "50000\n",
      "Suicide_Squad_(Film)_de.wikipedia.org_all-access_spider\n"
     ]
    }
   ],
   "source": [
    "data0 = data.copy()\n",
    "data0.iloc[:,1:].isnull().sum(axis=0).describe()\n",
    "data0 = data0.fillna(0)\n",
    "for idx in [0, 50000]:\n",
    "    print(idx)\n",
    "    print(data0.iloc[idx,0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(0)\n",
    "y_train_true = data[list(data.columns[-120:-60])]\n",
    "y_validate_true = data[list(data.columns[-60:])]\n",
    "data1 = data.copy()\n",
    "data1['index1'] = data1.index\n",
    "data1 = data1[['index1']+list(data1.columns)[1:-1]]\n",
    "data1[data1.columns[1:]] = data1[data1.columns[1:]].apply(lambda x: np.log(x+1))\n",
    "\n",
    "X_train = data1[data1.columns[1:-120]]\n",
    "y_train = data1[list(data1.columns[-120:-60])]\n",
    "X_validate = data1[list(data1.columns[61:-60])]\n",
    "y_validate = data1[list(data1.columns[-60:])]\n",
    "X_test = data1[list(data1.columns[121:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = (X_train - X_train.min())/(X_train.max()-X_train.min())\n",
    "X_validate_norm = (X_validate - X_validate.min())/(X_validate.max()-X_validate.min())\n",
    "X_test_norm = (X_test - X_test.min())/(X_test.max()-X_test.min())\n",
    "X_train1 = X_train_norm.values.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_validate1 = X_validate_norm.values.reshape(X_validate.shape[0], 1, X_validate.shape[1])\n",
    "y_train1 = y_train.values\n",
    "y_validate1 = y_validate.values\n",
    "X_test1 = X_test_norm.values.reshape(X_test.shape[0], 1, X_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Shadeh\\Anaconda3\\envs\\Shadehenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Shadeh\\Anaconda3\\envs\\Shadehenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Start training...\n",
      "WARNING:tensorflow:From C:\\Users\\Shadeh\\Anaconda3\\envs\\Shadehenv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 145063 samples, validate on 145063 samples\n",
      "Epoch 1/10\n",
      "145063/145063 [==============================] - 720s 5ms/step - loss: 0.6318 - val_loss: 0.6302\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63023, saving model to weights.best.from_scratch1.hdf5\n",
      "Epoch 2/10\n",
      "145063/145063 [==============================] - 127s 875us/step - loss: 0.5714 - val_loss: 0.6331\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.63023\n",
      "Epoch 3/10\n",
      "145063/145063 [==============================] - 129s 891us/step - loss: 0.5605 - val_loss: 0.8652\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.63023\n",
      "Epoch 4/10\n",
      "145063/145063 [==============================] - 133s 918us/step - loss: 0.5518 - val_loss: 0.5430\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63023 to 0.54302, saving model to weights.best.from_scratch1.hdf5\n",
      "Epoch 5/10\n",
      "145063/145063 [==============================] - 131s 905us/step - loss: 0.5448 - val_loss: 0.5102\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.54302 to 0.51016, saving model to weights.best.from_scratch1.hdf5\n",
      "Epoch 6/10\n",
      "145063/145063 [==============================] - 132s 907us/step - loss: 0.5386 - val_loss: 0.4959\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51016 to 0.49592, saving model to weights.best.from_scratch1.hdf5\n",
      "Epoch 7/10\n",
      "145063/145063 [==============================] - 43332s 299ms/step - loss: 0.5341 - val_loss: 0.5928\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.49592\n",
      "Epoch 8/10\n",
      "145063/145063 [==============================] - 16507s 114ms/step - loss: 0.5302 - val_loss: 0.5020\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.49592\n",
      "Epoch 9/10\n",
      "145063/145063 [==============================] - 218s 2ms/step - loss: 0.5271 - val_loss: 0.5583\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.49592\n",
      "Epoch 10/10\n",
      "145063/145063 [==============================] - 206s 1ms/step - loss: 0.5232 - val_loss: 0.5239\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.49592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x273f96b0550>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_model = Sequential()\n",
    "LSTM_model.add(LSTM(256, input_shape = (1,X_train.shape[1])))\n",
    "LSTM_model.add(Dropout(0.3))\n",
    "LSTM_model.add(Dense(60))\n",
    "LSTM_model.compile(loss = 'mean_absolute_error', optimizer = 'rmsprop')\n",
    "epochs = 10\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.from_scratch1.hdf5', verbose=1, save_best_only=True)\n",
    "print('Start training...')\n",
    "\n",
    "LSTM_model.fit(X_train1, y_train1, validation_data=(X_validate1, y_validate1), \n",
    "              epochs=epochs, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPE of validate set when training on X_train:  46.73116408514589\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LSTM_model.load_weights('weights.best.from_scratch1.hdf5') \n",
    "y_validate_pred = LSTM_model.predict(X_validate1)\n",
    "y_validate_pred1 = pd.DataFrame(data = y_validate_pred, columns = list(y_validate.columns))\n",
    "y_val_pred = y_validate_pred1.apply(lambda x: np.exp(x)-1)\n",
    "for i in list(y_val_pred.columns):\n",
    "    y_val_pred[i] = y_val_pred[i].apply(lambda x: round(x))\n",
    "\n",
    "y_val_pred[y_val_pred<0]=0\n",
    "print('SAMPE of validate set when training on X_train: ', smape_test(y_validate_true.stack(), y_val_pred.stack()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.96375802375267"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_true1 = y_train.copy()\n",
    "y_train_true1['V'] = y_train_true.median(axis=1)\n",
    "y_train_true1.head()\n",
    "\n",
    "for i in list(y_validate_true.columns):\n",
    "    y_train_true1[i] = y_train_true1['V']\n",
    "\n",
    "smape_test(y_validate_true.stack(), y_train_true1[y_train_true1.columns[-60:]].stack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    }
   ],
   "source": [
    "X_test_P = data[['Page']+list(data.columns[-7:])]\n",
    "LSTM_model.load_weights('weights.best.from_scratch1.hdf5')\n",
    "y_test_pred = LSTM_model.predict(X_test1)\n",
    "out_cols = list(pd.unique(key_split['date'].values))\n",
    "y_test_pred1 = pd.DataFrame(data = y_test_pred, columns = list(out_cols))\n",
    "y_test_pred2 = y_test_pred1.apply(lambda x: np.exp(x)-1)\n",
    "y_test_pred2[y_test_pred2<0] =0\n",
    "test_out = pd.merge(X_test_P, y_test_pred2, left_index = True, right_index = True)\n",
    "test_out1 = test_out[['Page']+list(test_out.columns.values[-60:])]\n",
    "test_out_120D = pd.melt(test_out1, id_vars='Page', var_name='date', value_name='Visits_120D')\n",
    "test_out_120D.head()\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.from_scratch2.hdf5', verbose=1, save_best_only=True)\n",
    "print('Start training...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 137809 samples, validate on 7254 samples\n",
      "Epoch 1/10\n",
      "    96/137809 [..............................] - ETA: 140:05:03 - loss: 0.5558"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shadeh\\Anaconda3\\envs\\Shadehenv\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.124005). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137809/137809 [==============================] - 1195s 9ms/step - loss: 0.5394 - val_loss: 0.5910\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59105, saving model to weights.best.from_scratch2.hdf5\n",
      "Epoch 2/10\n",
      "137809/137809 [==============================] - 165s 1ms/step - loss: 0.5306 - val_loss: 0.5375\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.59105 to 0.53751, saving model to weights.best.from_scratch2.hdf5\n",
      "Epoch 3/10\n",
      "137809/137809 [==============================] - 182s 1ms/step - loss: 0.5260 - val_loss: 0.5282\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.53751 to 0.52823, saving model to weights.best.from_scratch2.hdf5\n",
      "Epoch 4/10\n",
      "137809/137809 [==============================] - 176s 1ms/step - loss: 0.5218 - val_loss: 0.5000\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52823 to 0.50000, saving model to weights.best.from_scratch2.hdf5\n",
      "Epoch 5/10\n",
      "137809/137809 [==============================] - 181s 1ms/step - loss: 0.5182 - val_loss: 0.6103\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.50000\n",
      "Epoch 6/10\n",
      "137809/137809 [==============================] - 171s 1ms/step - loss: 0.5153 - val_loss: 0.5191\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.50000\n",
      "Epoch 7/10\n",
      "137809/137809 [==============================] - 170s 1ms/step - loss: 0.5124 - val_loss: 0.5771\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.50000\n",
      "Epoch 8/10\n",
      "137809/137809 [==============================] - 176s 1ms/step - loss: 0.5099 - val_loss: 0.4851\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.50000 to 0.48507, saving model to weights.best.from_scratch2.hdf5\n",
      "Epoch 9/10\n",
      "137809/137809 [==============================] - 170s 1ms/step - loss: 0.5071 - val_loss: 0.5261\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48507\n",
      "Epoch 10/10\n",
      "137809/137809 [==============================] - 161s 1ms/step - loss: 0.5048 - val_loss: 0.4797\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.48507 to 0.47967, saving model to weights.best.from_scratch2.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x273f9b19748>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_model.fit(X_validate1, y_validate1, validation_split = 0.05, \n",
    "               epochs=epochs, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model.load_weights('weights.best.from_scratch2.hdf5')\n",
    "\n",
    "### check model on training data\n",
    "y_train_pred = LSTM_model.predict(X_train1)\n",
    "y_train_pred1 = pd.DataFrame(data = y_train_pred, columns = list(y_train.columns))\n",
    "\n",
    "y_train_pred = y_train_pred1.apply(lambda x: np.exp(x)-1)\n",
    "\n",
    "\n",
    "for i in list(y_train_pred.columns):\n",
    "    y_train_pred[i] = y_train_pred[i].apply(lambda x: round(x))\n",
    "\n",
    "y_train_pred[y_train_pred<0]=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page</th>\n",
       "      <th>date</th>\n",
       "      <th>Visits_60D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2NE1_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>22.242765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2PM_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>25.247850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3C_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>5.454515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4minute_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>18.027788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>13.765117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Page        date  Visits_60D\n",
       "0            2NE1_zh.wikipedia.org_all-access_spider  2017-01-01   22.242765\n",
       "1             2PM_zh.wikipedia.org_all-access_spider  2017-01-01   25.247850\n",
       "2              3C_zh.wikipedia.org_all-access_spider  2017-01-01    5.454515\n",
       "3         4minute_zh.wikipedia.org_all-access_spider  2017-01-01   18.027788\n",
       "4  52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...  2017-01-01   13.765117"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_model.load_weights('weights.best.from_scratch2.hdf5')\n",
    "\n",
    "y_test_pred = LSTM_model.predict(X_test1)\n",
    "\n",
    "out_cols = list(pd.unique(key_split['date'].values))\n",
    "y_test_pred1 = pd.DataFrame(data = y_test_pred, columns = list(out_cols))\n",
    "\n",
    "##Inverse if outlier removed\n",
    "y_test_pred2 = y_test_pred1.apply(lambda x: np.exp(x)-1)\n",
    "\n",
    "y_test_pred2[y_test_pred2<0] =0\n",
    "\n",
    "###merge into Page\n",
    "test_out = pd.merge(X_test_P, y_test_pred2, left_index = True, right_index = True)\n",
    "test_out1 = test_out[['Page']+list(test_out.columns.values[-60:])]\n",
    "test_out_60D = pd.melt(test_out1, id_vars='Page', var_name='date', value_name='Visits_60D')\n",
    "test_out_60D.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b2efb1119663>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_out_120D\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_out_60D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Visits_Seq'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Visits_120D'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Visits_60D'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m## Merge on 'Page' and 'date'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mkey_rnn_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey_split\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Shadehenv\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3589\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3590\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3591\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3593\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = test_out_120D.merge(test_out_60D)\n",
    "results['Visits_Seq'] = results[['Visits_120D', 'Visits_60D']].mean(axis =1).apply(lambda x: round(x))\n",
    "\n",
    "## Merge on 'Page' and 'date'\n",
    "key_rnn_result = key_split.merge(results, how ='left')\n",
    "key_rnn_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
